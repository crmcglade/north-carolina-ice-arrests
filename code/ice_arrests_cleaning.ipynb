{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-KxxFJ_HGhU"
   },
   "source": [
    "## How many people has ICE arrested in North Carolina?\n",
    "\n",
    "The answer to this question is not simple. Nearly 20% of the rows in the ICE arrests database are missing values for State of Apprehension. So one cannot simply filter for North Carolina and call it a day. For the rows missing state values, you need to use a combination of the Area of Responsibility and the Apprehension Site Landmark to find everyone.\n",
    "\n",
    "This colab notebook goes through each step to catch as many North Carolina arrests as possible.\n",
    "\n",
    "The original files can be found here. https://drive.google.com/drive/folders/1ANA4ibvcbvohOfuXWv4uqaUDaLmQwIoz?usp=sharing\n",
    "\n",
    "The source is the Deportation Data Project. https://deportationdata.org/data.html \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 426,
     "status": "ok",
     "timestamp": 1760735174657,
     "user": {
      "displayName": "Caitlin McGlade",
      "userId": "01120879737236398881"
     },
     "user_tz": 240
    },
    "id": "g6uNvWOPkMT0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 111316,
     "status": "ok",
     "timestamp": 1760735286445,
     "user": {
      "displayName": "Caitlin McGlade",
      "userId": "01120879737236398881"
     },
     "user_tz": 240
    },
    "id": "eyOkCDcskP1P"
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('/Users/cmcglade/Documents/Projects/north-carolina-ice-arrests/og_data/ice_release_11aug2025/2025-ICLI-00019_2024-ICFO-39357_ERO Admin Arrests_LESA-STU_FINAL Redacted_raw.xlsx', skiprows=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 324,
     "status": "ok",
     "timestamp": 1760735351263,
     "user": {
      "displayName": "Caitlin McGlade",
      "userId": "01120879737236398881"
     },
     "user_tz": 240
    },
    "id": "rHhRuz6uY6uT",
    "outputId": "74639025-a8e7-4743-c266-1c32402fd562"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with missing location during my compare times: (1831, 24)\n"
     ]
    }
   ],
   "source": [
    "#create a column that flags Jan 20 through June in both 2024 and 2025\n",
    "\n",
    "df['comparetime'] = np.where(\n",
    "    df['Apprehension Date'].between('2025-01-20', '2025-06-30'),\n",
    "    'this_year',\n",
    "    np.where(\n",
    "        df['Apprehension Date'].between('2024-01-20', '2024-06-30'),\n",
    "        'last_year',\n",
    "        'not in compare range'\n",
    "    )\n",
    ")\n",
    "\n",
    "df = df[(df['comparetime'] == 'this_year') | (df['comparetime'] == 'last_year')]\n",
    "noloc = df[\n",
    "    df['Apprehension AOR'].isnull() &\n",
    "    df['Apprehension State'].isnull() &\n",
    "    df['Apprehension Site Landmark'].isnull()\n",
    "]\n",
    "\n",
    "print('Rows with missing location during my compare times:', noloc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "executionInfo": {
     "elapsed": 2040,
     "status": "ok",
     "timestamp": 1760461580245,
     "user": {
      "displayName": "Caitlin McGlade",
      "userId": "01120879737236398881"
     },
     "user_tz": 240
    },
    "id": "771BdSQRpu3I",
    "outputId": "af23cc65-98a8-45a3-e213-4be2b7b393a0"
   },
   "outputs": [],
   "source": [
    "## remove possible duplicates\n",
    "\n",
    "df = df.sort_values(['Unique Identifier', 'Apprehension Date'])\n",
    "\n",
    "# Only apply to rows with valid IDs\n",
    "has_id = df[df['Unique Identifier'].notna()].copy()\n",
    "no_id = df[df['Unique Identifier'].isna()].copy()\n",
    "\n",
    "# Calculate time differences per person\n",
    "has_id['time_diff'] = has_id.groupby('Unique Identifier')['Apprehension Date'].diff()\n",
    "\n",
    "# Flag rows that are within 24 hours of the previous record\n",
    "has_id['within_24h'] = has_id['time_diff'] <= pd.Timedelta(hours=24)\n",
    "\n",
    "# Keep the first record, and discard later ones within 24h of the same person\n",
    "# (i.e., keep where within_24h is False or NaN)\n",
    "df_kept = has_id[(~has_id['within_24h']) | (has_id['within_24h'].isna())]\n",
    "df_discarded = has_id[has_id['within_24h']]\n",
    "\n",
    "\n",
    "# Add back rows with missing IDs (we don't filter those)\n",
    "df = pd.concat([df_kept, no_id], ignore_index=True)\n",
    "\n",
    "df['approx_age'] = 2025-df['Birth Year']\n",
    "\n",
    "#create a year column\n",
    "df['year'] = df['Apprehension Date'].dt.year\n",
    "df['year'] = df['year'].astype(str)\n",
    "\n",
    "#create a month column\n",
    "df['month'] = df['Apprehension Date'].dt.month\n",
    "df['month'] = df['month'].astype(str)\n",
    "\n",
    "#create a generic day column so I can group by month-year\n",
    "df['day'] = '01'\n",
    "\n",
    "#create a standardized date for month-year\n",
    "df['standardized_date'] = df['year']+'-'+df['month']+'-'+df['day']\n",
    "df['standardized_date'] = pd.to_datetime(df['standardized_date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 233,
     "status": "ok",
     "timestamp": 1760462348225,
     "user": {
      "displayName": "Caitlin McGlade",
      "userId": "01120879737236398881"
     },
     "user_tz": 240
    },
    "id": "7KagVjjTunKc",
    "outputId": "4323a51c-538f-45ef-eb44-58a8895c85fd"
   },
   "outputs": [],
   "source": [
    "#create a method category \n",
    "df['method_cat'] = np.where(\n",
    "    df['Apprehension Method'].isin(['Non-Custodial Arrest', 'Located', 'Worksite Enforcement', 'Traffic Check', 'Crewman/Stowaway']),\n",
    "    'non-custodial, located, traffic check or worksite enforcement',\n",
    "    np.where(\n",
    "        df['Apprehension Method'] == 'Probation and Parole',\n",
    "        'probation and parole',\n",
    "        np.where(\n",
    "            df['Apprehension Method'].isin(['CAP Federal Incarceration', 'CAP State Incarceration', 'CAP Local Incarceration', 'Custodial Arrest']),\n",
    "            'federal, state or local incarceration',\n",
    "            np.where(\n",
    "                df['Apprehension Method'].isin([\n",
    "                    'Other efforts', 'Other Task Force', 'Other Agency (turned over to INS)',\n",
    "                    'Patrol Interior', 'Law Enforcement Agency Response Unit'\n",
    "                ]),\n",
    "                'other',\n",
    "                np.where(\n",
    "                    df['Apprehension Method'] == 'ERO Reprocessed Arrest',\n",
    "                    're-entry arrest',\n",
    "                    np.where(\n",
    "                        df['Apprehension Method'] == '287(g) Program',\n",
    "                        '287g',\n",
    "                        'other2'\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbGLEzjYpiOh"
   },
   "source": [
    "## Step One: Break the file into two pieces\n",
    "\n",
    "One file is missing state values and the other has them.\n",
    "\n",
    "Filter for NC and set aside the the clean file for later, move the messy file on to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 660,
     "status": "ok",
     "timestamp": 1760115980595,
     "user": {
      "displayName": "Caitlin McGlade",
      "userId": "01120879737236398881"
     },
     "user_tz": 240
    },
    "id": "6tJuM_o8K25m",
    "outputId": "ef8a8c63-3a6c-4ceb-a823-3155469f16fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No State:  (28810, 32)\n",
      "State:  (133892, 32)\n"
     ]
    }
   ],
   "source": [
    "# Lowercase columns first\n",
    "df['Apprehension Site Landmark'] = df['Apprehension Site Landmark'].str.lower()\n",
    "df['Apprehension State'] = df['Apprehension State'].str.lower()\n",
    "df['Apprehension AOR'] = df['Apprehension AOR'].str.lower()\n",
    "\n",
    "#create two separate files, one where there is no state and the other where there is. I will clean the no_state file.\n",
    "no_state = df[df['Apprehension State'].isnull()].copy()\n",
    "print('No State: ', no_state.shape)\n",
    "\n",
    "\n",
    "state = df[df['Apprehension State'].notna()].copy()\n",
    "print('State: ', state.shape)\n",
    "\n",
    "#filter for NC\n",
    "nc1 = state[state['Apprehension State']== 'north carolina']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3gM0iKwp_Ht"
   },
   "source": [
    "## Step Two: Peel states out of the site landmark column\n",
    "\n",
    "The site landmark column has a variety of location types. Sometimes, the state is referenced with a comma and the two letter abbreviation. Here, I specify valid state abbreviations and run a regex argument to pick up any of those and dump them into a new column called 'extracted state.'\n",
    "\n",
    "I then create a new file that grabs all rows where this was successful, filtered for nc and set that aside. I also create a file where there was not a state abbreviation within the landmark column, so the extracted state column is blank.\n",
    "\n",
    "That file, which I called no_state2 moves on to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 168,
     "status": "ok",
     "timestamp": 1760115980888,
     "user": {
      "displayName": "Caitlin McGlade",
      "userId": "01120879737236398881"
     },
     "user_tz": 240
    },
    "id": "eoEWUvTtLmrZ",
    "outputId": "ff0b3af8-b28e-429d-9715-bd393d3c6498"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with extracted states from Site Landmark:  (3613, 33)\n",
      "Rows where there is no Apprehension State and no clear state abbreviation in landmark:  (25197, 33)\n"
     ]
    }
   ],
   "source": [
    "# List of valid US state abbreviations\n",
    "us_states = [\n",
    "    'al','ak','az','ar','ca','co','ct','de','fl','ga','hi','id','il','in','ia','ks','ky','la',\n",
    "    'me','md','ma','mi','mn','ms','mo','mt','ne','nv','nh','nj','nm','ny','nc','nd','oh','ok',\n",
    "    'or','pa','ri','sc','sd','tn','tx','ut','vt','va','wa','wv','wi','wy'\n",
    "]\n",
    "\n",
    "# Extract potential 2-letter state code from \"Apprehension Site Landmark\"\n",
    "no_state['extracted_state'] = (\n",
    "    no_state['Apprehension Site Landmark']\n",
    "      .str.extract(r',\\s*([a-zA-Z]{2})(?![a-zA-Z])', expand=False)  # match exactly two letters after a comma\n",
    "      .str.lower()                                                  # make them lowercase\n",
    "      .where(lambda x: x.isin(us_states))                           # only keep valid US abbreviations\n",
    ")\n",
    "#create a dataframe that has state names now that didn't before\n",
    "extractedstate = no_state[no_state['extracted_state'].notna()].copy()\n",
    "print('Rows with extracted states from Site Landmark: ', extractedstate.shape)\n",
    "\n",
    "#create a dataframe that is still missing a state\n",
    "no_state2 = no_state[no_state['extracted_state'].isnull()].copy()\n",
    "print('Rows where there is no Apprehension State and no clear state abbreviation in landmark: ', no_state2.shape)\n",
    "\n",
    "extractedstate['Apprehension State'] = extractedstate['extracted_state']\n",
    "extractedstate = extractedstate.drop(columns=['extracted_state'])\n",
    "#extractedstate['Apprehension State'].unique()\n",
    "\n",
    "#filter for nc\n",
    "nc2 = extractedstate[extractedstate['Apprehension State'] == 'nc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Vp7LkHCrE0z"
   },
   "source": [
    "## Step Three: Search for North Carolina counties\n",
    "\n",
    "Using the file where there was no state abbreviation found, and no state referenced in the Apprehension State column, I create a new file that grabs any rows where the Area of Responsibility is Atlanta.\n",
    "\n",
    "[In a separate file](https://colab.research.google.com/drive/1644oPILC8rSwN2LFSWIYkOxGI0vIG85B?usp=sharing), I used the tigerline census shapefile to pull all the county names in North Carolina, Georgia and South Carolina (our AOR) and then ran a groupby to identify counties in North Carolina that share a name with counties in the other two states. I excluded those, and created a list of counties that only exist in North Carolina.\n",
    "\n",
    "From here, I created a new file called nc_county_names that grabs any row where the counties in my list are mentioned. I set that aside for later.\n",
    "\n",
    "I then also created a new file where those counties were not mentioned, and printed out the unique landmark variables and David Raynor looked at them and picked out several that were definitly in North Carolina, that I would not know about because I'm new here. I created a list of them, and then created a file that pulled those rows out called nc_stragglers.\n",
    "\n",
    "I had also created a file where there was no Apprehension AOR listed. I looked at the unique variables there and found one location that was definitely in North Carolina. Raynor found some that could possibly be, but we couldn't be sure, so I excluded them.\n",
    "\n",
    "The drawback of this is that I am likely missing people who were picked up in North Carolina, but simply because their county shares a name with another one in neighboring states, they are excluded. Or, in some places where the AOR is missing and the landmark doesn't provide obvious clues. There isn't really anything I can do about this, though, as there are not any other columns that would provide clues to confirm that the person was in North Carolina and not Georgia, or South Carolina.\n",
    "\n",
    "At the very end of all of this, I stack my three dataframes into one: the file that found North Carolina county names, the file that found places that are in North Carolina and are in the AOR, and the file that found a few rows that were in North Carolina but were missing an AOR.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1760115980930,
     "user": {
      "displayName": "Caitlin McGlade",
      "userId": "01120879737236398881"
     },
     "user_tz": 240
    },
    "id": "ftbqtrxQM8eW",
    "outputId": "b7db4271-4428-4518-8ffb-3eae537118fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows where AOR is Atlanta (368, 33)\n",
      "Rows where AOR is NULL (1537, 33)\n",
      "Rows in NC counties and are also in the Atlanta AOR (dupe name counties excluded: ) (39, 33)\n",
      "Rows that are in the Atlanta AOR but do not have any NC county names (329, 33)\n",
      "NC locations in Atlanta AOR that did not get picked up by county name:  (30, 33)\n",
      "NC locations that had no AOR at all:  (16, 33)\n",
      "Rows where AOR is Atlanta but there is no landmark info:  (4, 33)\n"
     ]
    }
   ],
   "source": [
    "#create a dataframe that might contain NC arrests\n",
    "potential_nc = no_state2[no_state2['Apprehension AOR'].str.contains('atlanta', na=False)]\n",
    "\n",
    "no_aor = no_state2[no_state2['Apprehension AOR'].isnull()]\n",
    "\n",
    "print('Rows where AOR is Atlanta', potential_nc.shape)\n",
    "print('Rows where AOR is NULL', no_aor.shape)\n",
    "\n",
    "nc_counties = [\n",
    "'alamance', 'alexander', 'alleghany', 'anson', 'ashe', 'avery',\n",
    "'bertie', 'bladen', 'brunswick', 'buncombe', 'cabarrus',\n",
    "'caldwell', 'carteret', 'caswell', 'catawba', 'chowan', 'cleveland',\n",
    "'columbus', 'craven', 'cumberland', 'currituck', 'dare', 'davidson',\n",
    "'davie', 'duplin', 'durham', 'edgecombe', 'gaston', 'gates', 'graham',\n",
    "'granville', 'guilford', 'halifax', 'harnett', 'haywood', 'henderson', 'hertford',\n",
    "'hoke', 'hyde', 'iredell', 'johnston', 'lenoir', 'martin', 'mcdowell', 'mecklenburg',\n",
    "'moore', 'nash', 'new hanover', 'northampton', 'onslow', 'orange', 'pamlico',\n",
    "'pasquotank', 'pender', 'perquimans', 'person', 'pitt', 'robeson',\n",
    "'rockingham', 'rowan', 'rutherford', 'sampson', 'scotland', 'stanly', 'stokes',\n",
    "'surry', 'swain', 'transylvania', 'tyrrell', 'vance', 'wake',\n",
    "'watauga', 'wilson', 'yadkin', 'yancey'\n",
    "\n",
    "]\n",
    "\n",
    "import re\n",
    "\n",
    "nc_county_names = potential_nc[\n",
    "    potential_nc['Apprehension Site Landmark']\n",
    "    .str.lower()\n",
    "    .str.contains('|'.join([re.escape(c) for c in nc_counties]), na=False)\n",
    "]\n",
    "\n",
    "import re\n",
    "\n",
    "other_potential_nc = potential_nc[\n",
    "    ~potential_nc['Apprehension Site Landmark']\n",
    "    .str.lower()\n",
    "    .str.contains('|'.join([re.escape(c) for c in nc_counties]), na=False)\n",
    "]\n",
    "\n",
    "\n",
    "print('Rows in NC counties and are also in the Atlanta AOR (dupe name counties excluded: )', nc_county_names.shape)\n",
    "print('Rows that are in the Atlanta AOR but do not have any NC county names', other_potential_nc.shape)\n",
    "\n",
    "\n",
    "\n",
    "defnc = [\n",
    "    'butner bop facility',\n",
    "    'nc department of corrections',\n",
    "    'rdu general area, non-specific',\n",
    "    'ero charlotte',\n",
    "    'edgecomb county jail',\n",
    "    'butner medical facility'\n",
    "]\n",
    "\n",
    "nc_stragglers = potential_nc[\n",
    "    potential_nc['Apprehension Site Landmark'].str.lower().isin(defnc)\n",
    "]\n",
    "\n",
    "nc_stragglers2 = no_aor[no_aor['Apprehension Site Landmark'] == 'sc. nc line from mecklenburg co e to richmond co; n to nc/va line']\n",
    "\n",
    "print('NC locations in Atlanta AOR that did not get picked up by county name: ', nc_stragglers.shape)\n",
    "print('NC locations that had no AOR at all: ', nc_stragglers2.shape)\n",
    "print('Rows where AOR is Atlanta but there is no landmark info: ', potential_nc[potential_nc['Apprehension Site Landmark'].isnull()].shape)\n",
    "\n",
    "#no_aor['Apprehension Site Landmark'].unique()\n",
    "\n",
    "nc3 = pd.concat([nc_stragglers, nc_stragglers2, nc_county_names])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-V2nluVuKPe"
   },
   "source": [
    "## Step Four: Putting it all together\n",
    "\n",
    "The first file I created (nc1) contained rows that all included the state name. The second file I created (nc2) has a state name as well, extracted from the landmark column.\n",
    "\n",
    "I filtered for North Carolina in both of these.\n",
    "\n",
    "The third file I created (nc3) combines rows where AOR is missing, but the landmark site had obvious ties to NC, AOR is Atlanta and the landmark had a NC-specific county name, AOR is Atlanta and the landmark had a place that was in North Carolina, identified by Raynor.\n",
    "\n",
    "I stack these files on top of each other to make a master file called nc.\n",
    "\n",
    "I have to do a little cleaning at this step, too.\n",
    "\n",
    "There are duplicate rows, so I drop those. And then, I need to drop the rows where ICE clearly made a mistake when they listed the arrest in North Carolina, because they have the area of responsibility in other places and the arrest landmark also in other places.\n",
    "\n",
    "The Deportation Data Project suggested people also dedupe rows where the apprehension date and the unique ID is repeated. I went a little further, noticing that some people have two arrests in one day, and decided to get rid of the second instance where someone was arrested twice in one day.\n",
    "\n",
    "I merge the groupby back to the nc file and then run a command to keep only the first iteration of each unique ID.\n",
    "\n",
    "I also discarded 36 rows where the same person was listed as arrested twice in the same month.\n",
    "\n",
    "You end up with 4,595 arrests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1760120412958,
     "user": {
      "displayName": "Caitlin McGlade",
      "userId": "01120879737236398881"
     },
     "user_tz": 240
    },
    "id": "IA7K9ADDrFKq",
    "outputId": "7c1b6eb0-881d-4f34-cb2f-6052cc88188b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2677, 33)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nc = pd.concat([nc1,nc2,nc3])\n",
    "nc = nc.drop_duplicates()\n",
    "nc = nc[\n",
    "    (nc['Apprehension AOR'] == 'atlanta area of responsibility') |\n",
    "    (nc['Apprehension AOR'].isnull())\n",
    "]\n",
    "\n",
    "nc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a file for cleaning\n",
    "landmarks = nc[['Apprehension Site Landmark']]\n",
    "landmarks = landmarks.drop_duplicates()\n",
    "landmarks.to_csv('/Users/cmcglade/Documents/Projects/north-carolina-ice-arrests/processed_data/landmarks.csv')\n",
    "\n",
    "#clean the county names in Excel\n",
    "#import the clean file back in\n",
    "clean_landmarks = pd.read_csv('/Users/cmcglade/Documents/Projects/north-carolina-ice-arrests/processed_data/landmarks-clean.csv')\n",
    "clean_landmarks.sample()\n",
    "\n",
    "#join the cleaned names to the original file\n",
    "\n",
    "nc = pd.merge(nc,clean_landmarks,how='left')\n",
    "\n",
    "#create a column for in custody or not \n",
    "\n",
    "nc['in_custody_arrest'] = np.where(nc['method_cat']=='non-custodial, located, traffic check or worksite enforcement', 'n', 'y')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "APUwwyoDt1HG"
   },
   "outputs": [],
   "source": [
    "nc.to_csv('/Users/cmcglade/Documents/Projects/north-carolina-ice-arrests/processed_data/nc_ice_arrests.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM7SyYm5jFjdF/NgvakXyeM",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
